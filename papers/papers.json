[
  {
    "id": "autoOp",
    "videoSrc": "papers/autoOp/autoOp.mp4",
    "title": "Autonomous Human-Robot Interaction via Operator Imitation",
    "authors": "Sammy Christen, David Müller, <strong>Agon Serifi</strong>, Ruben Grandia, Georg Wiedebach, Michael A. Hopkins, Espen Knoop, Moritz Bächer",
    "conference": "<em>In submission</em>",
    "links": [
      {
        "name": "project page",
        "url": "https://arxiv.org/abs/2504.02724"
      },
      {
        "name": "paper",
        "url": "https://arxiv.org/pdf/2504.02724"
      },
      {
        "name": "video",
        "url": "https://youtu.be/4U4etupwzhQ?si=zu2bujq642pwC1kI"
      },
      {
        "name": "bibtex",
        "url": "papers/autoOp/autoOp.txt"
      },
      {
        "name": "ArXiv",
        "url": "https://doi.org/10.48550/arXiv.2504.02724"
      }
    ],
    "description": "This paper presents a method for autonomously controlling expressive human-robot interactions by training a diffusion-based imitation model on real operator data. The resulting system predicts both continuous and discrete control inputs while conditioning on human pose, allowing the robot to produce robust, lifelike interactions and diverse moods that closely resemble expert operator performance in real-world scenarios."
  },
  {
    "id": "robotMDM",
    "videoSrc": "papers/robotMDM/robotMDM.mp4",
    "title": "Robot Motion Diffusion Model: Motion Generation for Robotic Characters",
    "authors": "<strong>Agon Serifi</strong>, Ruben Grandia, Espen Knoop, Markus Gross, Moritz Bächer",
    "conference": "<em>SIGGRAPH Asia</em> 2024",
    "links": [
      {
        "name": "project page",
        "url": "https://la.disneyresearch.com/publication/robot-motion-diffusion-model-motion-generation-for-robotic-characters/"
      },
      {
        "name": "paper",
        "url": "https://la.disneyresearch.com/wp-content/uploads/RobotMDM_red.pdf"
      },
      {
        "name": "video",
        "url": "https://www.youtube.com/watch?v=eRXS98c_Suc"
      },
      {
        "name": "bibtex",
        "url": "papers/robotMDM/robotMDM.txt"
      },
      {
        "name": "ACM",
        "url": "https://doi.org/10.1145/3680528.3687626"
      }
    ],
    "description": "This paper presents a method for aligning text-conditioned kinematic motion generators with the capabilities of robotic characters. Our approach combines motion generation with physics-based character control. First, we train a critic that acts as a reward surrogate by predicting the performance of the downstream non-differentiable control task. The critic provides an efficient and differentiable loss function, which is subsequently used to align the motion generator more closely with the character's physical capabilities."
  },
  {
    "id": "sbt",
    "videoSrc": "papers/sbt/sbt.mp4",
    "title": "Spline-Based Transformers",
    "authors": "Prashanth Chandran*<strong>, Agon Serifi*</strong>, Markus Gross, Moritz Bächer",
    "conference": "<em>ECCV</em> 2024 <strong>(Oral)</strong>",
    "equalContribution": "<em>*Equal contribution</em>",
    "links": [
      {
        "name": "project page",
        "url": "https://la.disneyresearch.com/publication/spline-based-transformers/"
      },
      {
        "name": "paper",
        "url": "https://la.disneyresearch.com/wp-content/uploads/SBT.pdf"
      },
      {
        "name": "supp",
        "url": "https://la.disneyresearch.com/wp-content/uploads/SBT_sup.pdf"
      },
      {
        "name": "video",
        "url": "https://www.youtube.com/watch?v=AzolLlIbKhg"
      },
      {
        "name": "bibtex",
        "url": "papers/sbt/sbt.txt"
      },
      {
        "name": "Springer",
        "url": "https://link.springer.com/chapter/10.1007/978-3-031-73016-0_1"
      }
    ],
    "description": "A new class of Transformer models that eliminate the need for positional encoding. Spline-based Transformers embed an input sequence of elements as a smooth trajectory in latent space. Fully leveraging the continuity inherent in sequential data, this approach significantly outperforms traditional transformer models in efficiency and accuracy."
  },
  {
    "id": "vmp",
    "videoSrc": "papers/vmp/vmp.mp4",
    "title": "VMP: Versatile Motion Priors for Robustly Tracking Motion on Physical Characters",
    "authors": "<strong>Agon Serifi</strong>, Ruben Grandia, Espen Knoop, Markus Gross, Moritz Bächer",
    "conference": "<em>SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)</em> 2024",
    "links": [
      {
        "name": "project page",
        "url": "https://la.disneyresearch.com/publication/vmp-versatile-motion-priors-for-robustly-tracking-motion-on-physical-characters/"
      },
      {
        "name": "paper",
        "url": "https://la.disneyresearch.com/wp-content/uploads/VMP_paper.pdf"
      },
      {
        "name": "video",
        "url": "https://www.youtube.com/watch?v=Q2I7u0tjlJs"
      },
      {
        "name": "bibtex",
        "url": "papers/vmp/vmp.txt"
      },
      {
        "name": "ACM",
        "url": "https://dl.acm.org/doi/10.1111/cgf.15175"
      }
    ],
    "description": "This paper introduces a two-stage framework for transferring expressive and dynamic motions onto robotic characters. Starting from a large corpus of human motion capture data, we first learn a latent space representation of short motion windows. This latent space then serves as a prior for training a tracking controller through reinforcement learning. The resulting controller pushes the boundaries of robotic characters, enabling them to move like humans and perform lifelike, dynamic motions such as dancing."
  },
  {
    "id": "neural_augmentation",
    "videoSrc": "papers/neural_augmentation/neural_augmentation.mp4",
    "title": "Transformer-Based Neural Augmentation of Robot Simulation Representations",
    "authors": "<strong>Agon Serifi</strong>, Espen Knoop, Christian Schumacher, Naveen Kumar, Markus Gross, Moritz Bächer",
    "conference": "<em>IEEE Robotics and Automation Letters, presented at IROS</em> 2023",
    "links": [
      {
        "name": "project page",
        "url": "https://la.disneyresearch.com/publication/transformer-based-neural-augmentation-of-robot-simulation-representations/"
      },
      {
        "name": "paper",
        "url": "https://la.disneyresearch.com/wp-content/uploads/Transformer-Based-Neural-Augmentation.pdf"
      },
      {
        "name": "video",
        "url": "https://www.youtube.com/watch?v=DMAg-I6F5XA"
      },
      {
        "name": "bibtex",
        "url": "papers/neural_augmentation/neural_augmentation.txt"
      },
      {
        "name": "IEEE",
        "url": "https://ieeexplore.ieee.org/document/10113169/similar#similar"
      }
    ],
    "description": "This paper introduces a modular approach for neural augmentation of simulation states, improving robot simulation accuracy by targeting individual building blocks such as actuators and rigid bodies separately."
  }
]